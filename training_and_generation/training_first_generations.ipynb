{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c5a8070",
   "metadata": {},
   "source": [
    "# Install and Import Dependencies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2863973",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install torch torchvision torchaudio\n",
    "# MacOS Binaries dont support CUDA, install from source if CUDA is needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0a4cd0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# install transformers library\n",
    "!pip3 install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0f5942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install natural language tool kit library\n",
    "!pip3 install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541931f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install pandas\n",
    "!pip3 install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6216c55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install seaborn\n",
    "!pip3 install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47f931af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#for data preprocessing\n",
    "from transformers import pipeline, GPT2Tokenizer\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, RandomSampler, SequentialSampler\n",
    "from csv import reader\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "import ast # for json string of list of dicts to list of dicts\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import pandas as pd\n",
    "import math\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65a3f9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the actual model\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "from transformers import GPT2LMHeadModel, GPT2Config, GPT2ForSequenceClassification\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "456d24db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for evaluation\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178b3ce6",
   "metadata": {},
   "source": [
    "# Custom Formatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6457dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to arrange subrecipe ingredients into groups by ingredient type\n",
    "def arrange_into_sub_groups (ingredients):\n",
    "    ing_list = []\n",
    "    for key in ingredients:\n",
    "      key_ = key.replace(':', '')\n",
    "      object = defaultdict(list)\n",
    "      for ing in ingredients[key][0]:\n",
    "        object[ing[\"type\"]].append(ing[\"ingredient\"])\n",
    "      ing_list.append({key_: dict(object)})\n",
    "    return ing_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c43d7f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class to embed tokens in each text\n",
    "class Formatter ():\n",
    "    \n",
    "    def _init_ (self):\n",
    "        result = None\n",
    "        raw = None\n",
    "    \n",
    "    def createInput (self):\n",
    "        input = self.raw[\"keywords\"]\n",
    "        self.result += \"<INPUT_START> \"\n",
    "        for i in range(4):\n",
    "          self.result += input[i]+ \" <NEXT_INPUT> \"\n",
    "        self.result += input[4]+ \" <INPUT_END> \"\n",
    "          \n",
    "    def process_ingredient_type(self, ingredients, typeOf):\n",
    "        self.result +=  \"<\"+typeOf+\"_START> \"\n",
    "        for i in range(0, len(ingredients)):\n",
    "            # missed \"recipe follows\" ingredients when scraping\n",
    "            if \"recipe follows\" in ingredients[i]:\n",
    "              continue\n",
    "            self.result +=  ingredients[i]\n",
    "            if i == len(ingredients)-1:\n",
    "                self.result += \" <\"+typeOf+\"_END> \"\n",
    "            else:\n",
    "                self.result +=  \" <NEXT_\"+typeOf+\"> \"\n",
    "        \n",
    "    def process_sub_group (self, group, typeOf):\n",
    "        self.result +=  \"<\"+typeOf+\"_INGREDIENTS_START> \"\n",
    "        if \"premade\" in group:\n",
    "            self.process_ingredient_type(group[\"premade\"], \"PREMADE\")\n",
    "        if \"prep\" in group:\n",
    "            self.process_ingredient_type(group[\"prep\"], \"PREP\")\n",
    "        if \"fat\" in group:\n",
    "            self.process_ingredient_type(group[\"fat\"], \"FAT\")\n",
    "        if \"structural\" in group:\n",
    "            self.process_ingredient_type(group[\"structural\"], \"STRUCTURAL\")\n",
    "        if \"moistening\" in group:\n",
    "            self.process_ingredient_type(group[\"moistening\"], \"MOISTENING\")\n",
    "        if \"sweetener\" in group:\n",
    "            self.process_ingredient_type(group[\"sweetener\"], \"SWEETENER\")\n",
    "        if \"leavener\" in group:\n",
    "            self.process_ingredient_type(group[\"leavener\"], \"LEAVENER\")\n",
    "        if \"flavoring\" in group:\n",
    "            self.process_ingredient_type(group[\"flavoring\"], \"FLAVORING\")\n",
    "        self.result +=  \"<\"+typeOf+\"_INGREDIENTS_END> \"\n",
    "        \n",
    "    def processLabel(self, text):\n",
    "        return re.sub(r\"[^a-zA-Z]+\", '', text).strip()\n",
    "        \n",
    "        \n",
    "    def createIngredients (self):\n",
    "        \n",
    "        ingredients = self.raw[\"ingredients\"]\n",
    "        self.result += \"<INGREDIENTS_START> \"\n",
    "        # group_names = [k for d in ingredients for k in d.keys()]\n",
    "        # add by group\n",
    "        for group in ingredients:\n",
    "          key = list(group.keys())[0]\n",
    "          self.process_sub_group(group[key], self.processLabel(key.upper()))\n",
    "        self.result += \"<INGREDIENTS_END> \"\n",
    "        \n",
    "            \n",
    "    def createSteps (self):\n",
    "        steps = self.raw[\"steps\"]\n",
    "        self.result += \"<STEP_START> \"\n",
    "        for i in range(0, len(steps)):\n",
    "            self.result += steps[i]\n",
    "            if i == len(steps)-1:\n",
    "                self.result += \" <STEP_END> \"\n",
    "            else:\n",
    "                self.result += \" <NEXT_STEP> \"    \n",
    "    \n",
    "    def createTitle (self):\n",
    "        self.result += \"<TITLE_START> \" + self.raw[\"title\"] + \" <TITLE_END> \"\n",
    "    \n",
    "    def createYield (self):\n",
    "        self.result += \"<YIELD_START> \" + self.raw[\"yield\"] + \" <YIELD_END> \"\n",
    "        \n",
    "    \n",
    "    def buildString (self):\n",
    "        self.result = \"<RECIPE_START> \"\n",
    "        self.createInput()\n",
    "        self.createYield()\n",
    "        self.createIngredients()\n",
    "        self.createSteps()\n",
    "        self.createTitle()\n",
    "        self.result += \"<RECIPE_END>\"\n",
    "    \n",
    "    def buildIngredientString(self):\n",
    "      self.createInput()\n",
    "      self.createYield()\n",
    "      self.createIngredients()\n",
    "\n",
    "    def buildStepString(self):\n",
    "      self.createInput()\n",
    "      self.createSteps()\n",
    "    \n",
    "    def run (self, object):\n",
    "        self.raw = object\n",
    "        self.buildString()\n",
    "  \n",
    "    def runIngredients (self, object):\n",
    "        self.raw = object\n",
    "        self.result = \"\"\n",
    "        self.buildIngredientString()\n",
    "    \n",
    "    def runSteps (self, object):\n",
    "        self.raw = object\n",
    "        self.result = \"\"\n",
    "        self.buildStepString()\n",
    "    \n",
    "    def getString (self):\n",
    "        return self.result\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b27317",
   "metadata": {},
   "source": [
    "# Custom Embed Tokens Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2785d6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\", do_lower_case=False)\n",
    "special_tokens = {\n",
    "    \"additional_special_tokens\": [\n",
    "        \"<TITLE_START>\",\n",
    "        \"<TITLE_END>\",\n",
    "        \"<YIELD_START>\",\n",
    "        \"<YIELD_END>\",\n",
    "        \"<STEP_START>\",\n",
    "        \"<NEXT_STEP>\",\n",
    "        \"<STEP_END>\",\n",
    "        \"<INGREDIENTS_START>\",\n",
    "        \"<NEXT_INGREDIENT>\",\n",
    "        \"<INGREDIENTS_END>\",\n",
    "        \"<PREMADE_START>\",\n",
    "        \"<NEXT_PREMADE>\",\n",
    "        \"<PREMADE_END>\",\n",
    "        \"<PREP_START>\",\n",
    "        \"<NEXT_PREP>\",\n",
    "        \"<PREP_END>\",\n",
    "        \"<STRUCTURAL_START>\",\n",
    "        \"<NEXT_STRUCTURAL>\",\n",
    "        \"<STRUCTURAL_END>\",\n",
    "        \"<FAT_START>\",\n",
    "        \"<NEXT_FAT>\",\n",
    "        \"<FAT_END>\",\n",
    "        \"<FLAVORING_START>\",\n",
    "        \"<NEXT_FLAVORING>\",\n",
    "        \"<FLAVORING_END>\",\n",
    "        \"<SWEETENER_START>\",\n",
    "        \"<NEXT_SWEETENER>\",\n",
    "        \"<SWEETENER_END>\",\n",
    "        \"<MOISTENING_START>\",\n",
    "        \"<NEXT_MOISTENING>\",\n",
    "        \"<MOISTENING_END>\",\n",
    "        \"<LEAVENER_START>\",\n",
    "        \"<NEXT_LEAVENER>\",\n",
    "        \"<LEAVENER_END>\",\n",
    "        \"<RECIPE_START>\",\n",
    "        \"<RECIPE_END>\",\n",
    "        \"<INPUT_START>\",\n",
    "        \"<INPUT_END>\",\n",
    "        \"<NEXT_INPUT>\",\n",
    "        \"<MAIN_INGREDIENTS_START>\",\n",
    "         \"<MAIN_INGREDIENTS_END>\",\n",
    "        \"<FROSTING_INGREDIENTS_START>\",\n",
    "         \"<FROSTING_INGREDIENTS_END>\",\n",
    "        \"<FILLING_INGREDIENTS_START>\",\n",
    "         \"<FILLING_INGREDIENTS_END>\",\n",
    "        \"<FROSTING_INGREDIENTS_START>\",\n",
    "         \"<FROSTING_INGREDIENTS_END>\",\n",
    "        \"<GANACHE_INGREDIENTS_START>\",\n",
    "         \"<GANACHE_INGREDIENTS_END>\",\n",
    "         \"<CURD_INGREDIENTS_START>\",\n",
    "         \"<CURD_INGREDIENTS_END>\",\n",
    "         \"<SYRUP_INGREDIENTS_START>\",\n",
    "         \"<SYRUP_INGREDIENTS_END>\",\n",
    "         \"<TOPPING_INGREDIENTS_START>\",\n",
    "         \"<TOPPING_INGREDIENTS_END>\",\n",
    "         \"<CRUST_INGREDIENTS_START>\",\n",
    "         \"<CRUST_INGREDIENTS_END>\",\n",
    "         \"<TOPPING_INGREDIENTS_START>\",\n",
    "         \"<TOPPING_INGREDIENTS_END>\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "my_tokenizer.add_special_tokens(special_tokens)\n",
    "my_tokenizer.add_special_tokens({'pad_token': '<PAD>'})\n",
    "\n",
    "end_token_id = my_tokenizer.convert_tokens_to_ids([\"<RECIPE_END>\"])[0]\n",
    "pad_token_id = my_tokenizer.convert_tokens_to_ids([\"<PAD>\"])[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02264f33",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "367e2978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# our data from csv\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/lail-lei/bim-gpt2-finetuning/main/data/baking_instructions_4852.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9f48240b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # method to create docs from csv\n",
    "def createDocs ():\n",
    "    \n",
    "  # create objects, and save to list\n",
    "  objects = []\n",
    "  # randomize \n",
    "  # The frac keyword argument specifies the fraction of rows to return in the random sample,  \n",
    "  # so frac=1 means return all rows (in random order).\n",
    "  # specifying drop=True prevents .reset_index from creating a column containing the old index entries.\n",
    "  random = df.sample(frac=1, random_state=42).reset_index(drop=True) \n",
    "\n",
    "    \n",
    "  for ind in random.index:\n",
    "    obj = {}\n",
    "    # for formatting\n",
    "    fm = Formatter()\n",
    "    obj[\"title\"] = random['Title'][ind]\n",
    "    obj[\"yield\"] = random['Yield'][ind]\n",
    "    obj[\"ingredients\"] = arrange_into_sub_groups(ast.literal_eval(random['processed_ingredients'][ind]))\n",
    "    obj[\"steps\"] = ast.literal_eval(random['Steps'][ind])\n",
    "    obj[\"keywords\"] = ast.literal_eval(random[\"keywords\"][ind]) \n",
    "    fm.run(obj) # format\n",
    "    length = len(fm.getString().split())\n",
    "    # save only the recipes less than or equal to\n",
    "    # the embed token limit for gpt2 (768)\n",
    "    if length <= 768:\n",
    "      objects.append(fm.getString()) #and append formatted obj\n",
    "  \n",
    "  return objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fff83c13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4452"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now this is a list of recipes with embed tokens \n",
    "docs = createDocs()\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4596476",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs[1004]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cfd4503",
   "metadata": {},
   "source": [
    "# Create Training and Testing Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d874043e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch size is the number of 768 token examples fed to the model\n",
    "# at each iteration\n",
    "batch_size = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4aefade",
   "metadata": {},
   "source": [
    "Every tensor passed to the model should be the same length. If the recipe is shorter \n",
    "than 768 tokens, it will be padded to a length of 768 using the padding token. \n",
    "In addition, an attention mask will be returned that needs to be passed to the model to tell it to ignore the padding tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd4bcc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2Dataset(Dataset):\n",
    "\n",
    "  # max length is number of embed tokens for gpt2 small\n",
    "  def __init__(self, txt_list, tokenizer, gpt2_type=\"gpt2\", max_length=768):\n",
    "\n",
    "    self.tokenizer = tokenizer\n",
    "    self.input_ids = []\n",
    "    self.attn_masks = []\n",
    "\n",
    "    for txt in txt_list:\n",
    "      encodings_dict = tokenizer(txt, truncation=True, max_length=max_length, padding=\"max_length\")\n",
    "      self.input_ids.append(torch.tensor(encodings_dict['input_ids']))\n",
    "      self.attn_masks.append(torch.tensor(encodings_dict['attention_mask']))\n",
    "    \n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.input_ids)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    return self.input_ids[idx], self.attn_masks[idx] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3834b4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset object loads training or test data into memory\n",
    "\n",
    "dataset = GPT2Dataset(docs, my_tokenizer, max_length=768)\n",
    "\n",
    "# Split into training and validation sets\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "print('{:>5,} training samples'.format(train_size))\n",
    "print('{:>5,} validation samples'.format(val_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baab7783",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d18461",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader object fetches data from a Dataset \n",
    "# and serves the data up in batches.\n",
    "\n",
    "# Create the DataLoaders for our training and validation datasets.\n",
    "# We'll take training samples in random order. \n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,  # The training samples.\n",
    "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
    "            batch_size = batch_size # Trains with this batch size.\n",
    "        )\n",
    "\n",
    "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
    "validation_dataloader = DataLoader(\n",
    "            val_dataset, # The validation samples.\n",
    "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
    "            batch_size = batch_size # Evaluate with this batch size.\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b6a204",
   "metadata": {},
   "source": [
    "# Finetuning the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e482bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # set configuration\n",
    "# configuration = GPT2Config.from_pretrained('gpt2', output_hidden_states=False)\n",
    "# # configuration.gradient_checkpointing: True\n",
    "# # configuration.n_head = 12\n",
    "# # configuration.n_layer = 12\n",
    "# # #increase dropout\n",
    "# # configuration.resid_pdrop = 0.25\n",
    "# # configuration.attn_pdrop = 0.25\n",
    "# # configuration.embd_pdrop = 0.25\n",
    "# configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6fd16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the model\n",
    "# The GPT2 Model transformer with a language modeling head on top \n",
    "# (linear layer with weights tied to the input embeddings).\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\", config=configuration)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54757c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this step is necessary because I've added some tokens (bos_token, etc) to the embeddings\n",
    "# otherwise the tokenizer and model tensors won't match up\n",
    "model.resize_token_embeddings(len(my_tokenizer))\n",
    "\n",
    "# fix model padding token id\n",
    "model.config.pad_token_id = pad_token_id\n",
    "#model.config.gradient_accumulation_steps = 10;\n",
    "\n",
    "# Tell pytorch to run this model on the GPU.\n",
    "device = torch.device(\"cuda\")\n",
    "model.cuda()\n",
    "\n",
    "# Set the seed value all over the place to make this reproducible.\n",
    "seed_val = 42\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e188d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some parameters I cooked up that work reasonably well\n",
    "epochs = 1\n",
    "learning_rate = 9e-4\n",
    "warmup_steps = 1e2\n",
    "epsilon = 1e-8\n",
    "\n",
    "# this produces sample output every 100 steps\n",
    "sample_every = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d84210a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = learning_rate,\n",
    "                  eps = epsilon,\n",
    "#                   weight_decay = 2,\n",
    "#                   correct_bias = False,\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08459bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total number of training steps is [number of batches] x [number of epochs]. \n",
    "# (Note that this is not the same as the number of training samples).\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "# This changes the learning rate as the training loop progresses\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = warmup_steps, \n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea195a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_time(elapsed):\n",
    "    return str(datetime.timedelta(seconds=int(round((elapsed)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f27fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_t0 = time.time()\n",
    "\n",
    "training_stats = []\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "for epoch_i in range(0, epochs):\n",
    "\n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    t0 = time.time() # reset timer\n",
    "\n",
    "    total_train_loss = 0 # reset training loss\n",
    "\n",
    "    model.train() # set model to \"training\" mode\n",
    "    \n",
    "    # Always clear any previously calculated gradients before performing a\n",
    "    # backward pass. PyTorch doesn't do this automatically because \n",
    "    # accumulating the gradients is \"convenient while training RNNs\". \n",
    "    # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
    "    model.zero_grad()   \n",
    "    \n",
    "    # keep track of ste\n",
    "    process_count = 0;\n",
    "\n",
    "    # for each step and batch (group of examples) in our training dataset\n",
    "    # steps = training samples / batch size \n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        \n",
    "#         print(step)\n",
    "        \n",
    "        # Unpack this training batch from our dataloader. As we unpack the batch,\n",
    "        # we'll also copy each tensor to the GPU using the `to` method.\n",
    "        #\n",
    "        # `batch` contains two pytorch tensors:\n",
    "        #   [0]: input ids /labels \n",
    "        #   [1]: attention masks\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_labels = batch[0].to(device)\n",
    "        b_masks = batch[1].to(device)\n",
    "        \n",
    "        \n",
    "        # Model arguments: \n",
    "\n",
    "        # The INPUT_IDS are often the only required parameters\n",
    "        # to be passed to the model as input. \n",
    "        # They are token indices, numerical representations of tokens \n",
    "        # building the sequences that will be used as input by the model.\n",
    "        # Tokenize text (products of gpt2 tokenizer) are converted into IDs\n",
    "        # which are understandable by the model. \n",
    "        # This can be done by directly feeding the sentence to the tokenizer, \n",
    "        # which leverages the Rust implementation of huggingface/tokenizers for peak performance.\n",
    "        \n",
    "        # LABELS are an optional argument \n",
    "        # which can be passed in order for the model to compute the loss itself. \n",
    "        # These labels should be the expected prediction of the model: \n",
    "        # it will use the standard loss in order to compute the loss \n",
    "        # between its predictions and the expected value (the label).\n",
    "        \n",
    "        # Since BIM is working on generation, the input ids and \n",
    "        # the labels are the same thing- the predicted value of the generated text\n",
    "        # is the generated text itself \n",
    "\n",
    "        # The ATTENTION_MASK argument indicates to the model \n",
    "        # which tokens should be attended to, and which should not.\n",
    "        # Since each training \"sample\" in the batch must be the same length (768 tokens)\n",
    "        # shorter samples included special padding tokens which will be ignored by the model\n",
    "        # The attention mask, b_masks, is a binary tensor \n",
    "        # indicating the position of the padded indices (and other tokens) \n",
    "        # so that the model does not attend to them.\n",
    "        \n",
    "        # Here, we perform a forward pass (evaluate the model on this training batch).\n",
    "        # returns a tuple containing various elements depending on the configuration.\n",
    "        # Based on current config, outputs include:\n",
    "        # language modeling loss, prediction scores of the language modeling head,\n",
    "        # and past: a tensor that contains pre-computed \n",
    "        # hidden-states (key and values in the attention blocks). \n",
    "        # Can be used (see past input) to speed up sequential decoding. \n",
    "        # When past is used as input, the input_ids which have their past \n",
    "        # given to this model should not be passed as input_ids \n",
    "        # as they have already been computed.\n",
    "        outputs = model(  b_input_ids,\n",
    "                          labels=b_labels, \n",
    "                          attention_mask = b_masks,\n",
    "                          token_type_ids=None\n",
    "                        )\n",
    "\n",
    "      \n",
    "        loss = outputs[0]  \n",
    "        \n",
    "        # Accumulate the training loss over all of the batches so that we can\n",
    "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
    "        # single value; the `.item()` function just returns the Python value \n",
    "        # from the tensor.\n",
    "        batch_loss = loss.item()\n",
    "        total_train_loss += batch_loss\n",
    "\n",
    "        # Get sample every x batches.\n",
    "        if step % sample_every == 0 and not step == 0:\n",
    "\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            print('  Batch {:>5,}  of  {:>5,}. Loss: {:>5,}.   Elapsed: {:}.'.format(step, len(train_dataloader), batch_loss, elapsed))\n",
    "\n",
    "            model.eval() # set mode to evaluate \n",
    "            # now we generate a sample\n",
    "            \n",
    "            sample_outputs = model.generate(\n",
    "                                    bos_token_id=random.randint(1,30000), #The id of the beginning-of-sequence token.\n",
    "                                    do_sample=True,   \n",
    "                                    top_k=50, \n",
    "                                    max_length = 200,\n",
    "                                    top_p=0.95, \n",
    "                                    num_return_sequences=1\n",
    "                                )\n",
    "            for i, sample_output in enumerate(sample_outputs):\n",
    "                  print(\"{}: {}\".format(i, my_tokenizer.decode(sample_output, skip_special_tokens=True)))\n",
    "            \n",
    "            model.train() # now let's set mode back to train\n",
    "\n",
    "        # backward pass- compute the gradients for each parameter from loss\n",
    "        loss.backward() \n",
    "        process_count += 1\n",
    "        \n",
    "        if process_count == batch_size:\n",
    "            process_count = 0    \n",
    "            optimizer.step()  # update the parameters using gradients computed in backward pass\n",
    "            scheduler.step() # change the learning rate as the training loop progresses\n",
    "                            # warm up vs actual training \n",
    "            optimizer.zero_grad()\n",
    "            model.zero_grad() # Reset gradients tensors\n",
    "        \n",
    "# if we are at end of batch, we can do gradients (if (step+1) % accumulation_steps == 0:  \n",
    "#optimizer.step() scheduler.step() model.zero_grad() \n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)       \n",
    "    \n",
    "    # Measure how long this epoch took.\n",
    "    training_time = format_time(time.time() - t0)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epoch took: {:}\".format(training_time))\n",
    "        \n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    t0 = time.time() # reset timer\n",
    "\n",
    "    model.eval() # set mode to evaluate \n",
    "    \n",
    "    # set variables to 0\n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in validation_dataloader:\n",
    "        \n",
    "        b_input_ids = batch[0].to(device) # get inputs\n",
    "        b_labels = batch[0].to(device) # get labels\n",
    "        b_masks = batch[1].to(device) # get mask\n",
    "        \n",
    "        # no gradients! Disabling gradient calculation is useful for inference, \n",
    "        # when you are sure that you will not call Tensor.backward().\n",
    "        with torch.no_grad():        \n",
    "            # input the inputs \n",
    "            outputs  = model(b_input_ids, \n",
    "#                            token_type_ids=None, \n",
    "                             attention_mask = b_masks,\n",
    "                            labels=b_labels)\n",
    "            # get the losss\n",
    "            loss = outputs[0]  \n",
    "            \n",
    "        batch_loss = loss.item()\n",
    "        total_eval_loss += batch_loss        \n",
    "\n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "    \n",
    "    validation_time = format_time(time.time() - t0)    \n",
    "\n",
    "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "    print(\"  Validation took: {:}\".format(validation_time))\n",
    "\n",
    "    # Record all statistics from this epoch.\n",
    "    training_stats.append(\n",
    "        {\n",
    "            'epoch': epoch_i + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Valid. Loss': avg_val_loss,\n",
    "            'Training Time': training_time,\n",
    "            'Validation Time': validation_time,\n",
    "            \"Perplexity\": math.exp(loss)\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    print(\"Perplexity:\", math.exp(loss))\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n",
    "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df1f497",
   "metadata": {},
   "source": [
    "Let's view the summary of the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2f5b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display floats with two decimal places.\n",
    "pd.set_option('precision', 3)\n",
    "\n",
    "# Create a DataFrame from our training statistics.\n",
    "df_stats = pd.DataFrame(data=training_stats)\n",
    "\n",
    "# Use the 'epoch' as the row index.\n",
    "df_stats = df_stats.set_index('epoch')\n",
    "\n",
    "# A hack to force the column headers to wrap.\n",
    "#df = df.style.set_table_styles([dict(selector=\"th\",props=[('max-width', '70px')])])\n",
    "\n",
    "# Display the table.\n",
    "df_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e834bc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#perplexity score\n",
    "math.exp(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53763f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use plot styling from seaborn.\n",
    "sns.set(style='darkgrid')\n",
    "\n",
    "# Increase the plot size and font size.\n",
    "sns.set(font_scale=1.5)\n",
    "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
    "\n",
    "# Plot the learning curve.\n",
    "plt.plot(df_stats['Training Loss'], 'b-o', label=\"Training\")\n",
    "plt.plot(df_stats['Valid. Loss'], 'g-o', label=\"Validation\")\n",
    "plt.plot(df_stats['Perplexity'], 'r-o', label=\"Perplexity\")\n",
    "\n",
    "# Label the plot.\n",
    "plt.title(\"Training, Validation Loss and Perplexity\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.xticks([1, 2])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77e0228",
   "metadata": {},
   "source": [
    "# Model Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70c335e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all of the model's parameters as a list of tuples.\n",
    "params = list(model.named_parameters())\n",
    "\n",
    "print('The GPT-2 model has {:} different named parameters.\\n'.format(len(params)))\n",
    "\n",
    "print('==== Embedding Layer ====\\n')\n",
    "\n",
    "for p in params[0:2]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== First Transformer ====\\n')\n",
    "\n",
    "for p in params[2:14]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== Output Layer ====\\n')\n",
    "\n",
    "for p in params[-2:]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109ba287",
   "metadata": {},
   "source": [
    "# Saving and Loading the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25032a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n",
    "\n",
    "output_dir = './model_80-20-train/'\n",
    "\n",
    "# # Create output directory if needed\n",
    "# if not os.path.exists(output_dir):\n",
    "#     os.makedirs(output_dir)\n",
    "\n",
    "print(\"Saving model to %s\" % output_dir)\n",
    "\n",
    "# Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
    "# They can then be reloaded using `from_pretrained()`\n",
    "model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
    "model_to_save.save_pretrained(output_dir)\n",
    "my_tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "# Good practice: save your training arguments together with the trained model\n",
    "# torch.save(args, os.path.join(output_dir, 'training_args.bin'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89296c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !ls -l --block-size=K ./model_80-20-train/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80eb8d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !ls -l --block-size=M ./model_80-20-train/pytorch_model.bin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd2059b",
   "metadata": {},
   "source": [
    "# LOAD an existing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4fe894c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50313, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50313, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dir = './model_80-20-train/'\n",
    "#Load a trained model and vocabulary that you have fine-tuned\n",
    "model = GPT2LMHeadModel.from_pretrained(output_dir)\n",
    "my_tokenizer = GPT2Tokenizer.from_pretrained(output_dir)\n",
    "\n",
    "# Tell pytorch to run this model on the GPU.\n",
    "device = torch.device(\"cuda\")\n",
    "model.cuda()\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ddd0a7",
   "metadata": {},
   "source": [
    "# Generate Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3f3dd22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: cupcakesapplecheeselemonjuice24 cupcakes1 1/2 sticks unsalted butter at room temperature2 1/2 cups all purpose flour4 large eggs at room temperature1 cup milk1 cup sugar1 1/4 teaspoons baking powder1/4 teaspoon fine salt1 teaspoon pure vanilla extract4 ounces cream cheese at room temperature1/4 cup sugar1/4 teaspoon grated lemon zest2 teaspoons pure vanilla extract1 teaspoon lemon juice2 sticks unsalted butter at at room temperature2 sticks unsalted butter at medium speed4 cups confectioners sugar3 tablespoons lemon juice1 tablespoon pure vanilla extractPreheat the oven to 350 degrees F. Line 24 muffin tins with paper liners.Put the flour salt baking powder and 3/4 cup sugar in a large bowl and whisk with an electric mixer until combined.Add the butter and beat until light and fluffy about 3 minutes.Add the eggs one at a time beating well after each addition.Add the vanilla and lemon juice and beat until incorporated.Reduce the mixer speed to low and gradually add the flour mixture beating until just combined.Fold in the milk and vanilla until just combined.Divide the batter among the prepared muffin cups filling them 3/4 of the way full.Bake until the tops spring back when gently pressed about 25 minutes.Cool the cupcakes in the tin on a rack for 10 minutes.Remove from the tin and cool completely on the rack.Meanwhile make the frosting Beat the butter and cream cheese in a large bowl with an electric mixer on medium speed until smooth.Reduce the speed to low and gradually beat in the confectioners sugar until incorporated.Increase the speed to medium and beat until smooth.Transfer 1/2 the frosting to a pastry bag fitted with a large star tip.Pipe or pipe the frosting onto the cupcakes using the pastry bag fitted with a large round tip.Top each cupcake with a lemon zest and lemon juice.Garnish with the candied fruit.Apple Cupcakes with Lemon Cream Cheese FrostingApple CupcakesFor the Cupcakes Preheat the oven to 350 degrees.Put the butter in a medium bowl and beat with an electric mixer until smooth.Add the sugar and beat until smooth.Add the cream cheese and beat until fluffy.Reduce the speed of the mixer to low add the lemon zest and vanilla and beat until fluffy.Add the lemon juice and beat until combined.Add the lemon zest and beat until well blended.Divide the frosting into 4 pieces and color one pink pink one yellow with a few shades of green.Place one pink green color into the center of each.Color one yellow one with a few shades of green frosting.Place the second pink color into the center of each and place the other pink into the center of the second.Fill the center of the third with some lemon frosting and place the third pink in the center of the fourth.Frost the cupcakes with the frosting using the tip of a large offset spatula.Decorate the top of each cupcake with a lemon slice and some candied fruit.Lemon Apple CupcakesApple Cupcakes with Lemon Cream FrostingApple Cupcakes with Lemon FrostingApple CupcakesCupcakesApple Cupcakes with Lemon Apple Cream Cheese FrostingFor the best results always comes from the imagination.Lemon-Lemon Apple CupcakesCupcake with Apple Cream Cheese FrostingApple CupcakesFor the perfect lemon cupcakesApple Cupcake with Lemon Cream Cheese FrostingApple Cream Cheese FrostingFor the best success at Cooking with Apple-Lemon-Apple CupcakesLemon-Apple Cupcakes with Lemon Cream Filling\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "prompt = \"<RECIPE_START> <INPUT_START> cupcakes <NEXT_INPUT> \" \n",
    "generated = torch.tensor(my_tokenizer.encode(prompt)).unsqueeze(0)\n",
    "generated = generated.to(device)\n",
    "\n",
    "sample_outputs = model.generate(\n",
    "                                generated, \n",
    "                                #bos_token_id=random.randint(1,30000),\n",
    "                                do_sample=True,   \n",
    "                                top_k=75, \n",
    "                                max_length = 1024,\n",
    "                                top_p=0.95, \n",
    "                                num_return_sequences=1,\n",
    "                                no_repeat_ngram_size = 7, \n",
    "                                temperature=0.75 \n",
    "                                )\n",
    "\n",
    "\n",
    "for i, sample_output in enumerate(sample_outputs):\n",
    "  print(\"{}: {}\\n\\n\".format(i, my_tokenizer.decode(sample_output, skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "c8722e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded = []\n",
    "for item in sample_outputs:\n",
    "    decoded.append(my_tokenizer.decode(item, skip_special_tokens=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "65fd91e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dataframe = pd.DataFrame(data=decoded, columns = [\"generated\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185e125f",
   "metadata": {},
   "source": [
    "# Parser and Format for HungerRice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d66de94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parser superclass\n",
    "class Parser:\n",
    "  def get_string(self, text, pattern):\n",
    "    truncated = re.search(pattern, text)\n",
    "    if truncated == None: \n",
    "      return None\n",
    "    return truncated.group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "907f6dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ingredients Parser object extends Parser superclass\n",
    "class IngredientsParser(Parser):\n",
    "  def __init__(self, text):\n",
    "    self.ingredients = []\n",
    "    self.string = self.get_string(text, \"<INGREDIENTS_START>.*?<INGREDIENTS_END>\")\n",
    "  \n",
    "  # get the name of each ingredient list (main, frosting, filling, etc)\n",
    "  def get_group_names (self, string):\n",
    "    # get all ingredients_start tokens in list\n",
    "    names= re.findall(\"\\w+_INGREDIENTS_START\", string)\n",
    "    # isolate list name from token\n",
    "    return [re.sub(\"_INGREDIENTS_START\", \"\", name) for name in names]\n",
    "\n",
    "  # remove empty strings from array\n",
    "  def remove_empty_strings (self, item):\n",
    "    stripped = item.strip()\n",
    "    if  stripped == \"\":\n",
    "      return\n",
    "    else:\n",
    "      return stripped\n",
    "  # hungerrice uses abbreviations for units\n",
    "  def abbreviate_unit (self, unit):\n",
    "    unit = unit.lower()\n",
    "    if unit == \"pinches\":\n",
    "      return \"pinch\";\n",
    "    if unit == \"dashes\":\n",
    "      return \"dash\";\n",
    "    if unit == \"teaspoons\" or unit ==\"teaspoon\":\n",
    "      return \"tsp\";\n",
    "    if unit == \"tablespoon\" or unit == \"t\" or unit== \"tablespoons\":\n",
    "      return \"tbsp\";\n",
    "    if unit == \"cups\" or unit == \"cup\":\n",
    "        return \"cup\";\n",
    "    if unit == \"pint\" or unit == \"pints\":\n",
    "      return \"pt\";\n",
    "    if unit == \"quart\" or unit == \"quarts\":\n",
    "      return \"qt\";\n",
    "    if unit == \"gallon\" or unit == \"gallons\": \n",
    "      return \"gal\";\n",
    "    if unit == \"fluid ounce\" or unit == unit == \"fluid ounces\" or unit == \"floz\" or unit == \"fl oz\":\n",
    "      return \"fl oz (US)\"\n",
    "    if unit == \"ounce\" or unit == unit == \"ounces\":\n",
    "      return \"oz\"\n",
    "    if unit == \"pound\" or unit == \"pounds\":\n",
    "      return \"lbs\"\n",
    "    if unit == \"g\" or unit == \"gs\" or unit == \"grams\" or unit == \"gms\":\n",
    "      return \"gm\";\n",
    "    if unit == \"kilos\" or unit == \"kilograms\" or unit == \"kgs\":\n",
    "      return \"kg\";\n",
    "    if unit == \"milliliter\" or unit == \"milliliters\" or unit == \"millilitre\" or unit == \"millilitres\" or unit == \"ml\":\n",
    "      return \"mL\";\n",
    "    if unit == \"liter\" or unit == \"liters\" or unit == \"litre\" or unit == \"litres\" or unit == \"l\":\n",
    "      return \"L\";\n",
    "    \n",
    "    return unit\n",
    "\n",
    "  # convert ingredient string item to dict\n",
    "  def create_ingredient_object (self, item, group):\n",
    "    unit = None\n",
    "    amount = None\n",
    "    plus = None\n",
    "    name = \"\"\n",
    "\n",
    "    units = re.findall(\"cups*|pinche*s*|dashe*s*|pints*|tsp|teaspoons*|tbsp|tablespoons*|qt|quarts*|pt|pints*|gal|gallons*|fluid\\soz|fl\\soz|pounds*|lbs\\s|ounces*|oz|\\d+\\s*g\\s|gms*|grams*|kilos*|kilograms*|kg|\\sml\\s|\\sl\\s\", item, re.IGNORECASE)\n",
    "    # set unit to first found unit \n",
    "    if len(units) > 0:\n",
    "      unit = self.abbreviate_unit(units[0])\n",
    "    \n",
    "    # more than 1 unit, (1 cup 2 tablespoons), need a plus phrase\n",
    "    if len(units) == 2:\n",
    "      second_amount_pattern1 = \"plus\\s\\d/*\\d*\"\n",
    "      second_amount_pattern2 = \"and\\s\\d/*\\d*\"\n",
    "      second_amount_pattern3 = \"with\\s\\d/*\\d*\"\n",
    "      second_amount = re.search(f\"{second_amount_pattern2}|{second_amount_pattern3}|{second_amount_pattern1}\", item)\n",
    "      if second_amount != None:\n",
    "        second_amount = re.sub(\"plus\\s|and\\s|with\\s\", \"\", second_amount.group())\n",
    "        plus = f\"{second_amount} {units[1]}\"\n",
    "  \n",
    "    # different patterns amount can come in\n",
    "    amount_pattern1 = \"^\\d*/*\\d*\"\n",
    "    amount_pattern2 = \"^\\d/*\\d*\\sto\\s\\d/*\\d*\"\n",
    "    amount_pattern3 = \"^\\d/*\\d*\\sand\\s\\d/*\\d*\"\n",
    "    amount = re.search(f\"{amount_pattern2}|{amount_pattern3}|{amount_pattern1}\", item)\n",
    "    \n",
    "    if amount != None:\n",
    "      amount = amount.group()\n",
    "    \n",
    "    # get last unit or amount to find part where ingredient name begins\n",
    "    if len(units) > 0:\n",
    "      last_unit = units[len(units) - 1]\n",
    "      name = item.split(last_unit)[1].strip()\n",
    "      if plus != None:\n",
    "        name += f\", plus {plus}\"\n",
    "    elif amount != None:\n",
    "      try:\n",
    "        name = item.split(amount)[1].strip()\n",
    "      except ValueError:\n",
    "        name = item.strip()\n",
    "    # if no unit nor amount, entire string is ingredient name\n",
    "    else:\n",
    "      name = item.strip()  \n",
    "    \n",
    "    return {\"amount\": amount, \"unit\": unit, \"name\": name, \"group\": group.lower()}\n",
    "  \n",
    "\n",
    "  def get_ingredients(self):\n",
    "    return self.ingredients\n",
    "    \n",
    "  def run_parser (self):\n",
    "    # can't run if no ingredient string\n",
    "    if self.string == None:\n",
    "      return False;\n",
    "    # get group names \n",
    "    names = self.get_group_names(self.string)\n",
    "    # return false if no ingredients\n",
    "    if len(names) == 0:\n",
    "      return False;\n",
    "\n",
    "    # create an array encountered ingredients\n",
    "    for name in names:\n",
    "      # isolate substring including only list ingredients\n",
    "      pattern = f\"<{name}_INGREDIENTS_START>.*?<{name}_INGREDIENTS_END>\"\n",
    "      list_string = re.search(pattern, self.string)\n",
    "      # skip this loop iteration if pattern doesn't exist in ingredients string\n",
    "      if list_string is None:\n",
    "        continue;\n",
    "      else:\n",
    "        list_string = list_string.group()\n",
    "      # remove list start and end tokens\n",
    "      list_string = re.sub(f\"<{name}_INGREDIENTS_.*?>\", '', list_string)\n",
    "      #remove ingredient category tokens\n",
    "      list_array = re.split(\"<.*?> \", list_string)\n",
    "      list_array = list(filter(lambda x: x != None, [self.remove_empty_strings(item) for item in list_array]))      \n",
    "    \n",
    "      # create objects for each ingredient\n",
    "      for item in list_array:\n",
    "        obj = self.create_ingredient_object(item, name)\n",
    "        self.ingredients.append(obj)\n",
    "    \n",
    "    return True\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d94f07f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Steps Parser object extends Parser superclass\n",
    "class StepsParser(Parser):\n",
    "  def __init__(self, text):\n",
    "    self.steps = []\n",
    "    self.string = self.get_string(text, \"<STEP_START>.*?<STEP_END>\")\n",
    "  \n",
    "  def get_steps(self):\n",
    "    return self.steps\n",
    "\n",
    "  def run_parser(self):\n",
    "    # can't run if no ingredient string\n",
    "    if self.string == None:\n",
    "      return False;\n",
    "    self.steps = re.sub(\"\\s\\s+\", \"\\n\", re.sub(\"<.*?>\", \"\", self.string).strip()).split(\"\\n\")\n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d28d9e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Yields parser object extends parser superclass\n",
    "class YieldsParser(Parser):\n",
    "  def __init__(self, text):\n",
    "    self.amount = 12\n",
    "    self.label = None\n",
    "    self.string = self.get_string(text, \"<YIELD_START>.*?<YIELD_END>\")\n",
    "  \n",
    "  def get_yield(self):\n",
    "    return {\"amount\": self.amount, \"label\": self.label}\n",
    "\n",
    "  def run_parser(self):\n",
    "    # can't run if no ingredient string\n",
    "    if self.string == None:\n",
    "      return False;\n",
    "    \n",
    "    stripped = re.sub(\"<.*?>\", '', self.string).strip()\n",
    "    string_amount = re.search(\"^\\d+\", stripped)\n",
    "    if string_amount != None:\n",
    "       self.amount = int(string_amount.group())\n",
    "       string_label = re.sub(string_amount.group(), \"\", stripped)\n",
    "       if string_label != None and len(string_label) > 0:\n",
    "         self.label = string_label.strip()\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "799e39a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tags parser object extends parser superclass\n",
    "class TagsParser(Parser):\n",
    "  def __init__(self, text):\n",
    "    self.tags = [\"bim\", \"bot\"]\n",
    "    self.string = self.get_string(text, \"<INPUT_START>.*?<INPUT_END>\")\n",
    "  \n",
    "  def get_tags(self):\n",
    "    return self.tags\n",
    "  # tags must be hyphenated in hungerrice\n",
    "  def remove_characters(self, item):\n",
    "    return re.sub(\"\\W\", \"_\", item)\n",
    "\n",
    "  def run_parser(self):\n",
    "    # can't run if no ingredient string\n",
    "    if self.string == None:\n",
    "      return False;\n",
    "    \n",
    "    input = re.sub(\"\\s\\s+\", \"\\n\", re.sub(\"<.*?>\", \"\", self.string).strip()).split(\"\\n\")\n",
    "    input = [self.remove_characters(i) for i in input]\n",
    "    self.tags = [*self.tags, *input]\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "239dd00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converts recipe to hungerrice recipe object\n",
    "class RecipeParser(Parser):\n",
    "  \n",
    "  def __init__(self, text):\n",
    "    self.ingredients = []\n",
    "    self.steps = []\n",
    "    self.yields = None\n",
    "    self.tags = None\n",
    "    self.images = [{\"image_url\": \"https://hungerrice-images.s3.us-east-2.amazonaws.com/ee464b52-e778-4b69-a9ec-1a7329cb06ad\", \"position\": 0}]\n",
    "    self.title = f\"BIM GENERATED RECIPE-{time.time()}\"\n",
    "    self.notes = f\"This recipe was generated at {time.time()} by a bot named BIM-GPT2. Do you think BIM did a good job? Leave a rating below!\"\n",
    "    self.owner_id = 1053\n",
    "    self.status = 1\n",
    "    self.string = self.get_string(text, \"<RECIPE_START>.*?<RECIPE_END>\")\n",
    "\n",
    "  def parse_ingredients(self, string):\n",
    "    parser = IngredientsParser(string)\n",
    "    success = parser.run_parser()\n",
    "    if success:\n",
    "      self.ingredients = parser.get_ingredients()\n",
    "      return True\n",
    "    else: \n",
    "      return False\n",
    "\n",
    "  def parse_steps(self, string):\n",
    "    parser = StepsParser(string)\n",
    "    success = parser.run_parser()\n",
    "    if success:\n",
    "      self.steps = parser.get_steps()\n",
    "      return True\n",
    "    else: \n",
    "      return False\n",
    "\n",
    "  def parse_yield(self, string):\n",
    "    parser = YieldsParser(string)\n",
    "    success = parser.run_parser()\n",
    "    if success:\n",
    "      self.yields = parser.get_yield()\n",
    "      return True\n",
    "    else: \n",
    "      return False\n",
    "  \n",
    "  def parse_tags(self, string):\n",
    "    parser = TagsParser(string)\n",
    "    success = parser.run_parser()\n",
    "    if success:\n",
    "      self.tags = parser.get_tags()\n",
    "      return True\n",
    "    else: \n",
    "      return False\n",
    "\n",
    "  def extract_title (self, text):\n",
    "    title = re.search(\"<TITLE_START>.*?<TITLE_END>\", text)\n",
    "    if title == None:\n",
    "      return None;\n",
    "    title = title.group()\n",
    "    return re.sub(\"<.*?>\", '', title).strip()\n",
    "\n",
    "  def run_parser (self):\n",
    "    if self.string == None:\n",
    "      print(\"No parseable string found\")\n",
    "      return False;\n",
    "    if self.parse_ingredients(self.string) == False:\n",
    "      print(\"No parseable ingredients found\")\n",
    "      return False\n",
    "    if self.parse_steps(self.string) == False:\n",
    "      print(\"No parseable steps found\")\n",
    "      return False\n",
    "    \n",
    "    extracted_title = self.extract_title(self.string)\n",
    "    \n",
    "    if extracted_title != None:\n",
    "      self.title = extracted_title\n",
    "    \n",
    "    self.parse_yield(self.string)\n",
    "    self.parse_tags(self.string)\n",
    "\n",
    "    return True\n",
    "\n",
    "  \n",
    "  def get_recipe (self):\n",
    "    return {\"title\": self.title, \n",
    "            \"owner_id\": self.owner_id, \n",
    "            \"original_owner_id\": self.owner_id,\n",
    "            \"status\": self.status,\n",
    "            \"yield\": self.yields,\n",
    "            \"ingredients\": self.ingredients,\n",
    "            \"steps\": self.steps,\n",
    "            \"notes\": self.notes,\n",
    "            \"tags\" : self.tags\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "28741dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_recipe (text):\n",
    "  parse = RecipeParser(text)\n",
    "  success = parse.run_parser()\n",
    "  if success:\n",
    "    return parse.get_recipe()\n",
    "  else:\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "383c9fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed = sample_dataframe.generated.apply(parse_recipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "aaa4ab51",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dataframe[\"parsed\"] = parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "e88e09d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>generated</th>\n",
       "      <th>parsed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;RECIPE_START&gt; &lt;INPUT_START&gt; cupcakes &lt;NEXT_IN...</td>\n",
       "      <td>{'title': 'Strawberry Cupcakes with Creamy But...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;RECIPE_START&gt; &lt;INPUT_START&gt; cupcakes &lt;NEXT_IN...</td>\n",
       "      <td>{'title': 'Peanut Butter Cupcakes', 'owner_id'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;RECIPE_START&gt; &lt;INPUT_START&gt; cupcakes &lt;NEXT_IN...</td>\n",
       "      <td>{'title': 'Cupcake', 'owner_id': 1053, 'origin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;RECIPE_START&gt; &lt;INPUT_START&gt; cupcakes &lt;NEXT_IN...</td>\n",
       "      <td>{'title': 'Carrot Cake Cupcakes', 'owner_id': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;RECIPE_START&gt; &lt;INPUT_START&gt; cupcakes &lt;NEXT_IN...</td>\n",
       "      <td>{'title': 'Chocolate Cupcakes with Marshmallow...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>&lt;RECIPE_START&gt; &lt;INPUT_START&gt; cupcakes &lt;NEXT_IN...</td>\n",
       "      <td>{'title': 'Chocolate Spider Web Cupcakes', 'ow...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>&lt;RECIPE_START&gt; &lt;INPUT_START&gt; cupcakes &lt;NEXT_IN...</td>\n",
       "      <td>{'title': 'Coconut Cupcakes', 'owner_id': 1053...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>&lt;RECIPE_START&gt; &lt;INPUT_START&gt; cupcakes &lt;NEXT_IN...</td>\n",
       "      <td>{'title': 'Cherry Cupcakes', 'owner_id': 1053,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>&lt;RECIPE_START&gt; &lt;INPUT_START&gt; cupcakes &lt;NEXT_IN...</td>\n",
       "      <td>{'title': 'Candy Cane Cupcakes', 'owner_id': 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>&lt;RECIPE_START&gt; &lt;INPUT_START&gt; cupcakes &lt;NEXT_IN...</td>\n",
       "      <td>{'title': 'Lemon Cream Cupcakes Recipe', 'owne...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>70 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            generated  \\\n",
       "0   <RECIPE_START> <INPUT_START> cupcakes <NEXT_IN...   \n",
       "1   <RECIPE_START> <INPUT_START> cupcakes <NEXT_IN...   \n",
       "2   <RECIPE_START> <INPUT_START> cupcakes <NEXT_IN...   \n",
       "3   <RECIPE_START> <INPUT_START> cupcakes <NEXT_IN...   \n",
       "4   <RECIPE_START> <INPUT_START> cupcakes <NEXT_IN...   \n",
       "..                                                ...   \n",
       "65  <RECIPE_START> <INPUT_START> cupcakes <NEXT_IN...   \n",
       "66  <RECIPE_START> <INPUT_START> cupcakes <NEXT_IN...   \n",
       "67  <RECIPE_START> <INPUT_START> cupcakes <NEXT_IN...   \n",
       "68  <RECIPE_START> <INPUT_START> cupcakes <NEXT_IN...   \n",
       "69  <RECIPE_START> <INPUT_START> cupcakes <NEXT_IN...   \n",
       "\n",
       "                                               parsed  \n",
       "0   {'title': 'Strawberry Cupcakes with Creamy But...  \n",
       "1   {'title': 'Peanut Butter Cupcakes', 'owner_id'...  \n",
       "2   {'title': 'Cupcake', 'owner_id': 1053, 'origin...  \n",
       "3   {'title': 'Carrot Cake Cupcakes', 'owner_id': ...  \n",
       "4   {'title': 'Chocolate Cupcakes with Marshmallow...  \n",
       "..                                                ...  \n",
       "65  {'title': 'Chocolate Spider Web Cupcakes', 'ow...  \n",
       "66  {'title': 'Coconut Cupcakes', 'owner_id': 1053...  \n",
       "67  {'title': 'Cherry Cupcakes', 'owner_id': 1053,...  \n",
       "68  {'title': 'Candy Cane Cupcakes', 'owner_id': 1...  \n",
       "69  {'title': 'Lemon Cream Cupcakes Recipe', 'owne...  \n",
       "\n",
       "[70 rows x 2 columns]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "7b347b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_created = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "83da6d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dataframe.to_csv(f\"./generated_recipes/{time_created}.csv\", index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "167356f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized = pd.json_normalize(sample_dataframe['parsed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "4da4cc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized.to_csv(f\"./generated_recipes/{time_created}_normalized.csv\", index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837e83e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
